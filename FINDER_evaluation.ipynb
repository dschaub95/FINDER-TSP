{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('findervenv': conda)"
  },
  "interpreter": {
   "hash": "cac74b763dd6365b660360e42fb86b007d1646e45a2a765c06460ea0e8dba80b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from FINDER import FINDER\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsplib95\n",
    "import networkx as nx\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'help_func': 1, 'net_type': 'S2V-DQN', 'max_bp_iter': 3, 'aggregatorID': 0, 'node_init_dim': 6, 'edge_init_dim': 4, 'state_init_dim': 1, 'node_embed_dim': 64, 'edge_embed_dim': 64, 'embeddingMethod': 3, 'ignore_covered_edges': 1, 'selected_nodes_inclusion': 2, 'IsHuberloss': 0, 'BATCH_SIZE': 64, 'initialization_stddev': 0.01, 'MAX_ITERATION': 30000, 'LEARNING_RATE': 0.0001, 'Alpha': 0.001, 'save_interval': 300, 'num_env': 1, 'g_type': 'tsp_2d', 'NUM_MIN': 15, 'NUM_MAX': 20, 'n_generator': 1000, 'decoder': 0, 'REG_HIDDEN': 32, 'IsDoubleDQN': 0, 'N_STEP': 1, 'GAMMA': 0.1, 'UPDATE_TIME': 1000, 'eps_start': 1.0, 'eps_end': 1.0, 'eps_step': 10000.0, 'MEMORY_SIZE': 150000, 'reward_normalization': 'max', 'reward_sign': 1, 'valid_path': 'valid_sets/synthetic_nrange_15_20_200/', 'valid_scale_fac': 0.0001, 'n_valid': 200, 'IsPrioritizedSampling': 0, 'epsilon': 1e-07, 'alpha': 0.6, 'beta': 0.4, 'beta_increment_per_sampling': 0.001, 'TD_err_upper': 1.0}\n",
      "Sucessfully loaded key 'help_func' from external config file!\n",
      "Sucessfully loaded key 'net_type' from external config file!\n",
      "Sucessfully loaded key 'max_bp_iter' from external config file!\n",
      "Sucessfully loaded key 'aggregatorID' from external config file!\n",
      "Sucessfully loaded key 'node_init_dim' from external config file!\n",
      "Sucessfully loaded key 'edge_init_dim' from external config file!\n",
      "Sucessfully loaded key 'state_init_dim' from external config file!\n",
      "Sucessfully loaded key 'node_embed_dim' from external config file!\n",
      "Sucessfully loaded key 'edge_embed_dim' from external config file!\n",
      "Sucessfully loaded key 'embeddingMethod' from external config file!\n",
      "Sucessfully loaded key 'ignore_covered_edges' from external config file!\n",
      "Sucessfully loaded key 'selected_nodes_inclusion' from external config file!\n",
      "Sucessfully loaded key 'IsHuberloss' from external config file!\n",
      "Sucessfully loaded key 'BATCH_SIZE' from external config file!\n",
      "Sucessfully loaded key 'initialization_stddev' from external config file!\n",
      "Sucessfully loaded key 'MAX_ITERATION' from external config file!\n",
      "Sucessfully loaded key 'LEARNING_RATE' from external config file!\n",
      "Sucessfully loaded key 'Alpha' from external config file!\n",
      "Sucessfully loaded key 'save_interval' from external config file!\n",
      "Sucessfully loaded key 'num_env' from external config file!\n",
      "Sucessfully loaded key 'g_type' from external config file!\n",
      "Sucessfully loaded key 'NUM_MIN' from external config file!\n",
      "Sucessfully loaded key 'NUM_MAX' from external config file!\n",
      "Sucessfully loaded key 'n_generator' from external config file!\n",
      "Sucessfully loaded key 'decoder' from external config file!\n",
      "Sucessfully loaded key 'REG_HIDDEN' from external config file!\n",
      "Sucessfully loaded key 'IsDoubleDQN' from external config file!\n",
      "Sucessfully loaded key 'N_STEP' from external config file!\n",
      "Sucessfully loaded key 'GAMMA' from external config file!\n",
      "Sucessfully loaded key 'UPDATE_TIME' from external config file!\n",
      "Sucessfully loaded key 'eps_start' from external config file!\n",
      "Sucessfully loaded key 'eps_end' from external config file!\n",
      "Sucessfully loaded key 'eps_step' from external config file!\n",
      "Sucessfully loaded key 'MEMORY_SIZE' from external config file!\n",
      "Sucessfully loaded key 'reward_normalization' from external config file!\n",
      "Sucessfully loaded key 'reward_sign' from external config file!\n",
      "Sucessfully loaded key 'valid_path' from external config file!\n",
      "Sucessfully loaded key 'valid_scale_fac' from external config file!\n",
      "Sucessfully loaded key 'n_valid' from external config file!\n",
      "Sucessfully loaded key 'IsPrioritizedSampling' from external config file!\n",
      "Sucessfully loaded key 'epsilon' from external config file!\n",
      "Sucessfully loaded key 'alpha' from external config file!\n",
      "Sucessfully loaded key 'beta' from external config file!\n",
      "Sucessfully loaded key 'beta_increment_per_sampling' from external config file!\n",
      "Sucessfully loaded key 'TD_err_upper' from external config file!\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.trace is deprecated. Please use tf.linalg.trace instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "2021-07-11 21:00:56.399042: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-07-11 21:00:56.422406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3990685000 Hz\n",
      "2021-07-11 21:00:56.422898: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556b15ab7350 executing computations on platform Host. Devices:\n",
      "2021-07-11 21:00:56.422923: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2021-07-11 21:00:56.423791: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-11 21:00:56.435944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-11 21:00:56.436272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253\n",
      "pciBusID: 0000:01:00.0\n",
      "2021-07-11 21:00:56.436594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-07-11 21:00:56.437751: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2021-07-11 21:00:56.438696: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2021-07-11 21:00:56.439000: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2021-07-11 21:00:56.440285: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2021-07-11 21:00:56.441386: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2021-07-11 21:00:56.445033: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-07-11 21:00:56.445223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-11 21:00:56.445969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-11 21:00:56.446344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2021-07-11 21:00:56.446448: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2021-07-11 21:00:56.497670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-11 21:00:56.497695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2021-07-11 21:00:56.497701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2021-07-11 21:00:56.497890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-11 21:00:56.498408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-11 21:00:56.498862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-11 21:00:56.499194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2918 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)\n",
      "2021-07-11 21:00:56.500614: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556b19317680 executing computations on platform CUDA. Devices:\n",
      "2021-07-11 21:00:56.500629: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 970, Compute Capability 5.2\n",
      "WARNING:tensorflow:From /tmp/ipykernel_26649/3462738274.py:2: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "tf.float32 has type DType, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "tf.float32 has type DType, but expected one of: int, long, bool\n"
     ]
    }
   ],
   "source": [
    "config_path = 'best_models/tsp_2d/nrange_15_20_len_101203/config.txt'\n",
    "dqn = FINDER(config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'test_sets/synthetic_nrange_10_20_1000/'\n",
    "# data_dir = 'test_sets/synthetic_nrange_15_20_1000/'\n",
    "# data_dir = 'test_sets/synthetic_nrange_40_50_1000/'\n",
    "# data_dir = 'valid_sets/synthetic_nrange_10_20_200/'\n",
    "# data_dir = 'valid_sets/synthetic_nrange_15_20_200/'\n",
    "data_dir = 'test_sets/tsp_min-n=15_max-n=20_num-graph=1000_type=random/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "graph_list = []\n",
    "\n",
    "atoi = lambda text : int(text) if text.isdigit() else text\n",
    "natural_keys = lambda text : [atoi(c) for c in re.split('(\\d+)', text)]\n",
    "\n",
    "fnames = os.listdir(data_dir)\n",
    "fnames.sort(key=natural_keys)\n",
    "\n",
    "for fname in fnames:\n",
    "    try:\n",
    "        if not '.tsp' in fname or '.sol' in fname:\n",
    "            continue\n",
    "        problem = tsplib95.load(data_dir + fname)\n",
    "        graph_list.append(problem.get_graph())\n",
    "        \n",
    "    except:\n",
    "        print('Error, while loading file {}'.format(fname))\n",
    "print(len(graph_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges from one node to itself\n",
    "for g in graph_list:\n",
    "    ebunch=[(k,k) for k in range(len(g.nodes))]\n",
    "    g.remove_edges_from(ebunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale size of the graphs such that it fits into 0,1 square\n",
    "scale_factor = 1/1000000\n",
    "for g in graph_list:\n",
    "    for node in g.nodes:\n",
    "        g.nodes[node]['coord'] = np.array(g.nodes[node]['coord']) * scale_factor\n",
    "    for edge in g.edges:\n",
    "        g.edges[edge]['weight'] = g.edges[edge]['weight'] * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.01203\n1.01203\n1.01203\n1.013181\n1.013181\n1.013181\n1.013111\n1.013111\n1.013111\n1.048982\n1.048982\n1.048982\nnrange_15_20_iter_3300_len_101203.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_min = dqn.cfg['NUM_MIN']\n",
    "num_max = dqn.cfg['NUM_MAX']\n",
    "g_type = dqn.cfg['g_type']\n",
    "base_path = f'best_models/{g_type}/'\n",
    "best_tour_length = np.inf\n",
    "# file_endings = ['ckpt', 'csv', 'pyx']\n",
    "models = os.scandir(base_path)\n",
    "for model in models:\n",
    "    # print(model.name)\n",
    "    # res = [ele for ele in file_endings if(ele in model)]\n",
    "    # check whether we have file or folder, continue in case of file\n",
    "    if model.is_file():\n",
    "        print(model.name)\n",
    "        continue\n",
    "    new_base_path = base_path + model.name + '/'\n",
    "    for f in os.listdir(new_base_path):\n",
    "        # print(f)\n",
    "        nrange_str = 'nrange_{}_{}'.format(num_min, num_max)\n",
    "        if ('ckpt' not in f):# or (nrange_str not in f):\n",
    "            continue\n",
    "        # print(f)\n",
    "        f_len = f.split('_')[-1].split('.')[0]\n",
    "        tour_length = float(f_len)/(10**(len(f_len)-1))\n",
    "        print(tour_length)\n",
    "        if f_len[0] != '1':\n",
    "            continue\n",
    "            # norm_tour_length = tour_length/float(config['valid_sol'])\n",
    "        else:\n",
    "            norm_tour_length = tour_length\n",
    "        if norm_tour_length < best_tour_length:\n",
    "            best_model_file = '.'.join(f.split('.')[0:-1])\n",
    "            best_model_base_path = new_base_path\n",
    "            best_tour_length = tour_length\n",
    "print(best_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best_models/tsp_2d/nrange_15_20_len_101203/\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/findervenv/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from best_models/tsp_2d/nrange_15_20_len_101203/nrange_15_20_iter_3300_len_101203.ckpt\n",
      "model sucessfully restored from file\n"
     ]
    }
   ],
   "source": [
    "print(best_model_base_path)\n",
    "g_type = dqn.cfg['g_type']\n",
    "# load model into Finder\n",
    "best_model = dqn.LoadModel(model_path=best_model_base_path+best_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "solutions = []\n",
    "k = 0\n",
    "for g in graph_list:\n",
    "    len, sol, time = dqn.Evaluate(g)\n",
    "    lengths.append(len)\n",
    "    solutions.append(sol)\n",
    "    print(k)\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 23, 17, 35, 16, 2, 26, 22, 10, 29, 8, 30, 9, 7, 14, 21, 3, 4, 37, 6, 36, 32, 1, 18, 12, 24, 20, 5, 11, 25, 19, 15, 39, 27, 38, 34, 31, 28, 13, 33]\n6.3450000000000015\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.1382610922374465"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "true_lengths = None\n",
    "with open(data_dir+'lengths.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [float(line.split(':')[-1].strip()) for line in lines]\n",
    "true_lengths = lines\n",
    "\n",
    "print(solutions[0])\n",
    "print(lengths[0])\n",
    "np.mean([length[0]/length[1] for length in zip(lengths, true_lengths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_min = 40\n",
    "num_max = 50\n",
    "index = pd.RangeIndex(start=num_min, stop=num_max)\n",
    "sol_df = pd.DataFrame()\n",
    "idx = 0\n",
    "for fname in fnames:\n",
    "    if not 'tsp' in fname:\n",
    "        continue\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df[fname.split('.')[0]] = solutions[idx]\n",
    "    sol_df = pd.concat([sol_df,tmp_df.astype(int)], ignore_index=False, axis=1)\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model_file.split('.')[0]\n",
    "sol_df.to_csv('results/{}/solution_nrange_{}_{}_{}.csv'.format(data_dir.split(\"/\")[-1], num_min, num_max, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_df = pd.DataFrame()\n",
    "idx = 0\n",
    "for fname in fnames:\n",
    "    if not 'tsp' in fname:\n",
    "        continue\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df[fname.split('.')[0]] = [lengths[idx]]\n",
    "    lens_df = pd.concat([lens_df,tmp_df], ignore_index=False, axis=1)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_df.to_csv('results/{}/tour_lengths_nrange_{}_{}_{}.csv'.format(data_dir.split(\"/\")[-1], num_min, num_max, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_list[0]\n",
    "length, solution, time_ = dqn.Evaluate(g=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0, 6, 1, 11, 8, 9, 2, 7, 5, 10, 4, 3], 2.7711)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "solution, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether length calculation is correct\n",
    "def calc_tour_length(graph, solution):\n",
    "    tot_len = 0\n",
    "    for i in range(np.array(solution).shape[0]):\n",
    "        if i == np.array(solution).shape[0] - 1:\n",
    "            tot_len += graph[solution[i]][solution[0]]['weight']\n",
    "        else:\n",
    "            tot_len += graph[solution[i]][solution[i + 1]]['weight']\n",
    "    return tot_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node(graph, cur_sol, new_node):\n",
    "    cur_dist = 10000000.0\n",
    "    for i in range(0, np.array(cur_sol).shape[0]):\n",
    "        if i + 1 == np.array(cur_sol).shape[0]:\n",
    "            adj = cur_sol[0]\n",
    "        else:\n",
    "            adj = cur_sol[i + 1]\n",
    "        if cur_sol[i] == adj:\n",
    "            cost = graph[new_node][cur_sol[i]]['weight'] + graph[new_node][adj]['weight']\n",
    "        else:\n",
    "            cost = graph[new_node][cur_sol[i]]['weight'] + graph[new_node][adj]['weight'] - graph[cur_sol[i]][adj]['weight']\n",
    "        if cost < cur_dist:\n",
    "            cur_dist = cost\n",
    "            pos = i + 1\n",
    "    cur_sol.insert(pos, new_node)\n",
    "    return  cur_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_helper_tour(g):\n",
    "    cur_sol = [0]\n",
    "    rem_nodes = list(np.arange(g.number_of_nodes()))\n",
    "    rem_nodes.remove(0)\n",
    "    tour_length = 0\n",
    "    while np.array(cur_sol).shape[0] < g.number_of_nodes():\n",
    "        if np.array(rem_nodes).shape[0]-1 == 0:\n",
    "            rand_idx = 0\n",
    "        else:\n",
    "            rand_idx = np.random.randint(0, np.array(rem_nodes).shape[0]-1)\n",
    "        new_node = rem_nodes[rand_idx]\n",
    "        tour_length += add_node(g, cur_sol, new_node)\n",
    "        rem_nodes.remove(new_node)\n",
    "    return tour_length, cur_sol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result = -np.inf\n",
    "min_result = np.inf\n",
    "for i in range(100):\n",
    "    help_lengths = []\n",
    "    for graph in graph_list:\n",
    "        help_lengths.append(calc_helper_tour(graph)[0])\n",
    "\n",
    "    true_lengths = None\n",
    "    with open(data_dir+'lengths.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [float(line.split(':')[-1].strip()) for line in lines]\n",
    "    true_lengths = lines\n",
    "    result = np.mean([length[0]/length[1] for length in zip(help_lengths, true_lengths)])\n",
    "    if result < min_result:\n",
    "        min_result = result\n",
    "    if result > max_result:\n",
    "        max_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0291948212594315\n1.0353626379176066\n"
     ]
    }
   ],
   "source": [
    "print(min_result)\n",
    "print(max_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.9577"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": []
  }
 ]
}