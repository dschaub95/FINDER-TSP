import tensorflow as tf
from dqn.attention_module import MultiHeadAttention, create_padding_mask