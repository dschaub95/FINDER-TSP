{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Make predictions\n",
    "\n",
    "Now let's make some predictions on the validation dataset with the trained model. This notebook is designed to generate heat maps on TSP200, TSP500, TSP1000 and TSP10000."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from utils.plot_utils import plot_predictions_cluster\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Remove warning\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# from scipy.sparse import SparseEfficiencyWarning\n",
    "# warnings.simplefilter('ignore', SparseEfficiencyWarning)\n",
    "\n",
    "from utils.process import *\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from data.data_generator import tsp_instance_reader\n",
    "\n",
    "from utils.tsplib import read_tsplib_coor, read_tsplib_opt, write_tsplib_prob\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Loading trained Att-GCN based on TSP50-trainset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# model-parameter\n",
    "config_path = \"configs/tsp50.json\"\n",
    "config = get_config(config_path)\n",
    "\n",
    "# setting random seed to 1\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda\")\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "else:\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(1)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Instantiate the network\n",
    "net = nn.DataParallel(ResidualGatedGCNModel(config, dtypeFloat, dtypeLong))\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()  \n",
    "# Define optimizer\n",
    "learning_rate = config.learning_rate\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# Load checkpoint\n",
    "log_dir = f\"./logs/{config.expt_name}/\"\n",
    "if torch.cuda.is_available():\n",
    "    # TSP-50\n",
    "    checkpoint = torch.load(\"logs/tsp50/best_val_checkpoint.tar\")\n",
    "else:\n",
    "    checkpoint = torch.load(\"logs/tsp50/best_val_checkpoint.tar\", map_location='cpu')\n",
    "# Load network state\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Load optimizer state\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# Load other training parameters\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['train_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "for param_group in optimizer.param_groups:\n",
    "    learning_rate = param_group['lr']  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Graph Sampling "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_one_tsp(tsp_source, coor_buff, node_num=20, \n",
    "                    cluster_center = 0, top_k = 19, top_k_expand = 19):\n",
    "    mean_rank_sum, mean_greater_zero_edges = 0, 0\n",
    "    coor, opt = tsp_instance_reader(tspinstance=tsp_source,\n",
    "                       buff = coor_buff, num_node=node_num)\n",
    "    coors = [coor]\n",
    "    \n",
    "    distA = pdist(coors[0], metric='euclidean')\n",
    "    distB_raw = squareform(distA)\n",
    "    distB = squareform(distA) + 10.0 * np.eye(N = node_num, M =node_num, dtype = np.float64)\n",
    "    \n",
    "    edges_probs = np.zeros(shape = (node_num, node_num), dtype = np.float64)\n",
    "    \n",
    "    pre_edges = np.ones(shape = (top_k + 1, top_k + 1), dtype = np.int32) + np.eye(N = top_k + 1, M = top_k + 1)\n",
    "    pre_node = np.ones(shape = (top_k + 1, ))\n",
    "    \n",
    "    pre_node_target = np.arange(0, top_k + 1)\n",
    "    pre_node_target = np.append(pre_node_target, 0)\n",
    "    pre_edge_target = np.zeros(shape = (top_k + 1, top_k + 1)) \n",
    "    pre_edge_target[pre_node_target[:-1], pre_node_target[1:]] = 1\n",
    "    pre_edge_target[pre_node_target[1:], pre_node_target[:-1]] = 1\n",
    "    \n",
    "    neighbor = np.argpartition(distB, kth = top_k, axis=1)\n",
    "    \n",
    "    neighbor_expand = np.argpartition(distB, kth = top_k_expand, axis=1)\n",
    "    Omega_w = np.zeros(shape=(node_num, ), dtype = np.int32)\n",
    "    Omega = np.zeros(shape=(node_num, node_num), dtype = np.int32)\n",
    "    \n",
    "    edges, edges_values = [], []\n",
    "    nodes, nodes_coord = [], []\n",
    "    edges_target, nodes_target = [], []\n",
    "    meshs = []\n",
    "    num_clusters = 0\n",
    "    if node_num==20:\n",
    "        num_clusters_threshold = 1\n",
    "    else:\n",
    "        num_clusters_threshold = math.ceil((node_num / (top_k+1) ) * 5)\n",
    "    all_visited = False\n",
    "    num_batch_size = 0\n",
    "    \n",
    "    while num_clusters < num_clusters_threshold or all_visited == False:\n",
    "        if all_visited==False:\n",
    "            \n",
    "            cluster_center_neighbor = neighbor[cluster_center, :top_k]\n",
    "            cluster_center_neighbor = np.insert(cluster_center_neighbor,\n",
    "                                                0, cluster_center)\n",
    "        else:\n",
    "            np.random.shuffle(neighbor_expand[cluster_center, :top_k_expand])\n",
    "            cluster_center_neighbor = neighbor_expand[cluster_center, :top_k]\n",
    "            cluster_center_neighbor = np.insert(cluster_center_neighbor,\n",
    "                                                0, cluster_center)\n",
    "        \n",
    "        Omega_w[cluster_center_neighbor] += 1\n",
    "\n",
    "        # case 4\n",
    "        node_coord = coors[0][cluster_center_neighbor]\n",
    "        x_y_min = np.min(node_coord, axis=0)\n",
    "        scale = 1.0 / np.max(np.max(node_coord, axis=0)-x_y_min)\n",
    "        node_coord = node_coord - x_y_min\n",
    "        node_coord *= scale\n",
    "        nodes_coord.append(node_coord)\n",
    "\n",
    "        # case 1-2\n",
    "        edges.append(pre_edges)\n",
    "        mesh = np.meshgrid(cluster_center_neighbor, cluster_center_neighbor)\n",
    "        \n",
    "        edges_value = distB_raw[mesh].copy()\n",
    "        edges_value *= scale\n",
    "        edges_values.append(edges_value)\n",
    "        meshs.append(mesh)\n",
    "        Omega[mesh] += 1\n",
    "\n",
    "        # case 3\n",
    "        nodes.append(pre_node)\n",
    "\n",
    "        # case 5-6\n",
    "        edges_target.append(pre_edge_target)\n",
    "        nodes_target.append(pre_node_target[:-1])\n",
    "\n",
    "        num_clusters += 1\n",
    "        \n",
    "        if 0 not in Omega_w:\n",
    "            all_visited = True\n",
    "        \n",
    "        cluster_center = np.random.choice(np.where(Omega_w==np.min(Omega_w))[0])\n",
    "    \n",
    "    return edges, edges_values, nodes, nodes_coord, edges_target, nodes_target, meshs, Omega, opt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def multiprocess_write(sub_prob, meshgrid, omega, node_num = 20,\n",
    "                       tsplib_name = './sample.txt', statiscs = False, opt = None):\n",
    "    edges_probs = np.zeros(shape = (node_num, node_num), dtype = np.float32)\n",
    "    for i in range(len(meshgrid)):\n",
    "        edges_probs[list(meshgrid[i])] += sub_prob[i, :, :, 1]\n",
    "    edges_probs = edges_probs / (omega + 1e-8)#[:, None]\n",
    "    # normalize the probability in an instance \n",
    "    edges_probs = edges_probs + edges_probs.T\n",
    "    edges_probs_norm = edges_probs/np.reshape(np.sum(edges_probs, axis=1),\n",
    "                                              newshape=(node_num, 1))\n",
    "    \n",
    "    if statiscs:\n",
    "        mean_rank = 0\n",
    "        for i in range(node_num-1):\n",
    "            mean_rank += len(np.where(edges_probs_norm[opt[i], :]>=edges_probs_norm[opt[i], opt[i+1]])[0]) \n",
    "        mean_rank /= (node_num-1)\n",
    "        \n",
    "        false_negative_edge = opt[np.where(edges_probs_norm[opt[:-1], opt[1:]]<1e-5)]\n",
    "        # false negative edges in an instance\n",
    "        num_fne = len(false_negative_edge)\n",
    "        \n",
    "        greater_zero_edges = len(np.where(edges_probs_norm>1e-6)[0])\n",
    "        greater_zero_edges /= node_num\n",
    "        \n",
    "        write_tsplib_prob(tsplib_name, edge_prob = edges_probs_norm,\n",
    "                  num_node=node_num, mean=mean_rank, fnn = num_fne, greater_zero=greater_zero_edges)\n",
    "    else:\n",
    "        write_tsplib_prob(tsplib_name, edge_prob = edges_probs_norm,\n",
    "                          num_node=node_num, mean=0, fnn = 0, greater_zero=0)\n",
    "    return mean_rank\n",
    "net.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 Heatmap generator on TSP200\n",
    "\n",
    "All output would be stored on the dir `./results/heatmap/tsp200`. After running the next node code cell, we would get 128 probabilistic heat maps for TSP200-instances and then copy them to the dir `MCTS/tsp-200-500-1000/heatmap`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_nodes = 200\n",
    "f = open('./data/tsp{}_test_concorde.txt'.format(num_nodes), 'r')\n",
    "testset_tsp = f.readlines()\n",
    "f.close()\n",
    "\n",
    "config.expt_name = 'tsp{}'.format(num_nodes)\n",
    "K = 49\n",
    "K_expand = 79\n",
    "avg_mean_rank = [] \n",
    "top_k, cluster_center = K, 0\n",
    "batch_size = 128 \n",
    "threshold = math.ceil((num_nodes / (top_k+1) ) * 5)\n",
    "epoch = int(len(testset_tsp)/batch_size)\n",
    "buff_coor = np.zeros(shape=(num_nodes, 2), dtype = np.float64)\n",
    "start_row_num = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# init\n",
    "count_buff = np.zeros(shape=(batch_size*threshold, ), dtype=np.int32)\n",
    "edges = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "edges_values = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.float16)\n",
    "nodes = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "nodes_coord = np.zeros(shape = (batch_size*threshold, K+1, 2), dtype=np.float16)\n",
    "edges_target = np.zeros(shape = (batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "nodes_target = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "meshs = np.zeros(shape = (batch_size*threshold, 2, K+1, K+1), dtype=np.int32)\n",
    "Omegas = np.zeros(shape = (batch_size, num_nodes, num_nodes), dtype=np.int32)\n",
    "opts = np.zeros(shape = (batch_size, num_nodes+1), dtype=np.int32)\n",
    "num_neighbors = config.num_neighbors\n",
    "beam_size = config.beam_size\n",
    "\n",
    "sum_time = 0\n",
    "for j in tqdm.tqdm(range(epoch)):\n",
    "    start = time.time()\n",
    "    for i in range(batch_size):\n",
    "        edge, edges_value, node, node_coord, edge_target, node_target, mesh, omega, opt = test_one_tsp(tsp_source=testset_tsp[start_row_num+i], \n",
    "                                                                                      coor_buff=buff_coor, node_num=num_nodes, \n",
    "                                                                                      cluster_center=0, top_k=K, top_k_expand=K_expand)\n",
    "        edges[i*threshold:(i+1)*threshold, ...] = edge\n",
    "        edges_values[i*threshold:(i+1)*threshold, ...] = edges_value\n",
    "        nodes[i*threshold:(i+1)*threshold, ...] = node\n",
    "        nodes_coord[i*threshold:(i+1)*threshold, ...] = node_coord\n",
    "        edges_target[i*threshold:(i+1)*threshold, ...] = edge_target\n",
    "        nodes_target[i*threshold:(i+1)*threshold, ...] = node_target\n",
    "        meshs[i*threshold:(i+1)*threshold, ...] = mesh\n",
    "        Omegas[i, ...] = omega\n",
    "        opts[i, ...] = opt\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = Variable(torch.LongTensor(edges).type(dtypeLong), requires_grad=False)\n",
    "        x_edges_values = Variable(torch.FloatTensor(edges_values).type(dtypeFloat), requires_grad=False)\n",
    "        x_nodes = Variable(torch.LongTensor(nodes).type(dtypeLong), requires_grad=False)\n",
    "        x_nodes_coord = Variable(torch.FloatTensor(nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "        y_edges = Variable(torch.LongTensor(edges_target).type(dtypeLong), requires_grad=False)\n",
    "        y_nodes = Variable(torch.LongTensor(nodes_target).type(dtypeLong), requires_grad=False)\n",
    "\n",
    "        # Compute class weights\n",
    "        edge_labels = y_edges.cpu().numpy().flatten()\n",
    "        edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        y_preds_prob = F.softmax(y_preds, dim=3)\n",
    "        y_preds_prob_numpy = y_preds_prob.cpu().numpy()\n",
    "\n",
    "    # multi - processes\n",
    "#     progress_pool = Pool(processes=10)\n",
    "#     for i in range(batch_size):\n",
    "#         heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "#         progress_pool.apply_async(multiprocess_write, args=(y_preds_prob_numpy[i*thre:(i+1)*thre, ...],\n",
    "#                                                            meshs[i*thre:(i+1)*thre, ...], Omegas[i, ...],\n",
    "#                                                            num_nodes, heatmap_path, True, opts[i, ...]))\n",
    "#     progress_pool.close()\n",
    "#     progress_pool.join()\n",
    "    end = time.time()\n",
    "    sum_time += end - start\n",
    "    # single - process\n",
    "    for i in range(batch_size):\n",
    "        heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "        rank = multiprocess_write(y_preds_prob_numpy[i*threshold:(i+1)*threshold, ...],\n",
    "                                                           meshs[i*threshold:(i+1)*threshold, ...], Omegas[i, ...],\n",
    "                                                           num_nodes, heatmap_path, True, opts[i, ...])\n",
    "        avg_mean_rank.append(rank)\n",
    "    start_row_num+= batch_size\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2 Heatmap generator on TSP500\n",
    "\n",
    "All output would be stored on the dir `./results/heatmap/tsp500`. After running the next node code cell, we would get 128 probabilistic heat maps for TSP500-instances and then copy them to the dir `MCTS/tsp-200-500-1000/heatmap`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_nodes = 500\n",
    "f = open('./data/tsp{}_test_concorde.txt'.format(num_nodes), 'r')\n",
    "testset_tsp = f.readlines()\n",
    "f.close()\n",
    "\n",
    "config.expt_name = 'tsp{}'.format(num_nodes)\n",
    "K = 49\n",
    "avg_mean_rank = [] \n",
    "top_k, cluster_center = K, 0\n",
    "batch_size = 64 \n",
    "threshold = math.ceil((num_nodes / (top_k+1) ) * 5)\n",
    "epoch = int(len(testset_tsp)/batch_size)\n",
    "buff_coor = np.zeros(shape=(num_nodes, 2), dtype = np.float64)\n",
    "start_row_num = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# init\n",
    "K_expand = 99\n",
    "count_buff = np.zeros(shape=(batch_size*threshold, ), dtype=np.int32)\n",
    "edges = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "edges_values = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.float16)\n",
    "nodes = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "nodes_coord = np.zeros(shape = (batch_size*threshold, K+1, 2), dtype=np.float16)\n",
    "edges_target = np.zeros(shape = (batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "nodes_target = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "meshs = np.zeros(shape = (batch_size*threshold, 2, K+1, K+1), dtype=np.int32)\n",
    "Omegas = np.zeros(shape = (batch_size, num_nodes, num_nodes), dtype=np.int32)\n",
    "opts = np.zeros(shape = (batch_size, num_nodes+1), dtype=np.int32)\n",
    "num_neighbors = config.num_neighbors\n",
    "beam_size = config.beam_size\n",
    "\n",
    "sum_time = 0\n",
    "for j in tqdm.tqdm(range(epoch)):\n",
    "    start = time.time()\n",
    "    for i in range(batch_size):\n",
    "        edge, edges_value, node, node_coord, edge_target, node_target, mesh, omega, opt = test_one_tsp(tsp_source=testset_tsp[start_row_num+i], \n",
    "                                                                                      coor_buff=buff_coor, node_num=num_nodes, \n",
    "                                                                                      cluster_center=0, top_k=K, top_k_expand=K_expand)\n",
    "        edges[i*threshold:(i+1)*threshold, ...] = edge\n",
    "        edges_values[i*threshold:(i+1)*threshold, ...] = edges_value\n",
    "        nodes[i*threshold:(i+1)*threshold, ...] = node\n",
    "        nodes_coord[i*threshold:(i+1)*threshold, ...] = node_coord\n",
    "        edges_target[i*threshold:(i+1)*threshold, ...] = edge_target\n",
    "        nodes_target[i*threshold:(i+1)*threshold, ...] = node_target\n",
    "        meshs[i*threshold:(i+1)*threshold, ...] = mesh\n",
    "        Omegas[i, ...] = omega\n",
    "        opts[i, ...] = opt\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = Variable(torch.LongTensor(edges).type(dtypeLong), requires_grad=False)\n",
    "        x_edges_values = Variable(torch.FloatTensor(edges_values).type(dtypeFloat), requires_grad=False)\n",
    "        x_nodes = Variable(torch.LongTensor(nodes).type(dtypeLong), requires_grad=False)\n",
    "        x_nodes_coord = Variable(torch.FloatTensor(nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "        y_edges = Variable(torch.LongTensor(edges_target).type(dtypeLong), requires_grad=False)\n",
    "        y_nodes = Variable(torch.LongTensor(nodes_target).type(dtypeLong), requires_grad=False)\n",
    "\n",
    "        # Compute class weights\n",
    "        edge_labels = y_edges.cpu().numpy().flatten()\n",
    "        edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        y_preds_prob = F.softmax(y_preds, dim=3)\n",
    "        y_preds_prob_numpy = y_preds_prob.cpu().numpy()\n",
    "\n",
    "    # multi - processes\n",
    "#     progress_pool = Pool(processes=10)\n",
    "#     for i in range(batch_size):\n",
    "#         heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "#         progress_pool.apply_async(multiprocess_write, args=(y_preds_prob_numpy[i*thre:(i+1)*thre, ...],\n",
    "#                                                            meshs[i*thre:(i+1)*thre, ...], Omegas[i, ...],\n",
    "#                                                            num_nodes, heatmap_path, True, opts[i, ...]))\n",
    "#     progress_pool.close()\n",
    "#     progress_pool.join()\n",
    "    end = time.time()\n",
    "    sum_time += end - start\n",
    "    # single - process\n",
    "    for i in range(batch_size):\n",
    "        heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "        rank = multiprocess_write(y_preds_prob_numpy[i*threshold:(i+1)*threshold, ...],\n",
    "                                                           meshs[i*threshold:(i+1)*threshold, ...], Omegas[i, ...],\n",
    "                                                           num_nodes, heatmap_path, True, opts[i, ...])\n",
    "        avg_mean_rank.append(rank)\n",
    "    start_row_num+= batch_size\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3 Heatmap generator on TSP1000\n",
    "\n",
    "All output would be stored on the dir `./results/heatmap/tsp1000`. After running the next node code cell, we would get 128 probabilistic heat maps for TSP1000-instances and then copy them to the dir `MCTS/tsp-200-500-1000/heatmap`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_nodes = 1000\n",
    "f = open('./data/tsp{}_test_concorde.txt'.format(num_nodes), 'r')\n",
    "testset_tsp = f.readlines()\n",
    "f.close()\n",
    "\n",
    "config.expt_name = 'tsp{}'.format(num_nodes)\n",
    "K = 49\n",
    "avg_mean_rank = [] \n",
    "top_k, cluster_center = K, 0\n",
    "batch_size = 64 \n",
    "threshold = math.ceil((num_nodes / (top_k+1) ) * 5)\n",
    "epoch = int(len(testset_tsp)/batch_size)\n",
    "buff_coor = np.zeros(shape=(num_nodes, 2), dtype = np.float64)\n",
    "start_row_num = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# init\n",
    "K_expand = 99\n",
    "count_buff = np.zeros(shape=(batch_size*threshold, ), dtype=np.int32)\n",
    "edges = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "edges_values = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.float16)\n",
    "nodes = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "nodes_coord = np.zeros(shape = (batch_size*threshold, K+1, 2), dtype=np.float16)\n",
    "edges_target = np.zeros(shape = (batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "nodes_target = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "meshs = np.zeros(shape = (batch_size*threshold, 2, K+1, K+1), dtype=np.int32)\n",
    "Omegas = np.zeros(shape = (batch_size, num_nodes, num_nodes), dtype=np.int32)\n",
    "opts = np.zeros(shape = (batch_size, num_nodes+1), dtype=np.int32)\n",
    "num_neighbors = config.num_neighbors\n",
    "beam_size = config.beam_size\n",
    "\n",
    "sum_time = 0\n",
    "for j in tqdm.tqdm(range(epoch)):\n",
    "    start = time.time()\n",
    "    for i in range(batch_size):\n",
    "        edge, edges_value, node, node_coord, edge_target, node_target, mesh, omega, opt = test_one_tsp(tsp_source=testset_tsp[start_row_num+i], \n",
    "                                                                                      coor_buff=buff_coor, node_num=num_nodes, \n",
    "                                                                                      cluster_center=0, top_k=K, top_k_expand=K_expand)\n",
    "        edges[i*threshold:(i+1)*threshold, ...] = edge\n",
    "        edges_values[i*threshold:(i+1)*threshold, ...] = edges_value\n",
    "        nodes[i*threshold:(i+1)*threshold, ...] = node\n",
    "        nodes_coord[i*threshold:(i+1)*threshold, ...] = node_coord\n",
    "        edges_target[i*threshold:(i+1)*threshold, ...] = edge_target\n",
    "        nodes_target[i*threshold:(i+1)*threshold, ...] = node_target\n",
    "        meshs[i*threshold:(i+1)*threshold, ...] = mesh\n",
    "        Omegas[i, ...] = omega\n",
    "        opts[i, ...] = opt\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = Variable(torch.LongTensor(edges).type(dtypeLong), requires_grad=False)\n",
    "        x_edges_values = Variable(torch.FloatTensor(edges_values).type(dtypeFloat), requires_grad=False)\n",
    "        x_nodes = Variable(torch.LongTensor(nodes).type(dtypeLong), requires_grad=False)\n",
    "        x_nodes_coord = Variable(torch.FloatTensor(nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "        y_edges = Variable(torch.LongTensor(edges_target).type(dtypeLong), requires_grad=False)\n",
    "        y_nodes = Variable(torch.LongTensor(nodes_target).type(dtypeLong), requires_grad=False)\n",
    "\n",
    "        # Compute class weights\n",
    "        edge_labels = y_edges.cpu().numpy().flatten()\n",
    "        edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        y_preds_prob = F.softmax(y_preds, dim=3)\n",
    "        y_preds_prob_numpy = y_preds_prob.cpu().numpy()\n",
    "\n",
    "    # multi - processes\n",
    "#     progress_pool = Pool(processes=10)\n",
    "#     for i in range(batch_size):\n",
    "#         heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "#         progress_pool.apply_async(multiprocess_write, args=(y_preds_prob_numpy[i*thre:(i+1)*thre, ...],\n",
    "#                                                            meshs[i*thre:(i+1)*thre, ...], Omegas[i, ...],\n",
    "#                                                            num_nodes, heatmap_path, True, opts[i, ...]))\n",
    "#     progress_pool.close()\n",
    "#     progress_pool.join()\n",
    "    end = time.time()\n",
    "    sum_time += end - start\n",
    "    # single - process\n",
    "    for i in range(batch_size):\n",
    "        heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "        rank = multiprocess_write(y_preds_prob_numpy[i*threshold:(i+1)*threshold, ...],\n",
    "                                                           meshs[i*threshold:(i+1)*threshold, ...], Omegas[i, ...],\n",
    "                                                           num_nodes, heatmap_path, True, opts[i, ...])\n",
    "        avg_mean_rank.append(rank)\n",
    "    start_row_num+= batch_size\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4 Heatmap generator on TSP10000\n",
    "\n",
    "All output would be stored on the dir `./results/heatmap/tsp10000`. After running the next node code cell, we would get 16 probabilistic heat maps for TSP10000-instances and then copy them to the dir `MCTS/tsp-10000/heatmap`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_nodes = 10000\n",
    "f = open('./data/tsp{}_test_concorde.txt'.format(num_nodes), 'r')\n",
    "testset_tsp = f.readlines()\n",
    "f.close()\n",
    "\n",
    "config.expt_name = 'tsp{}'.format(num_nodes)\n",
    "K = 49\n",
    "avg_mean_rank = [] \n",
    "top_k, cluster_center = K, 0\n",
    "batch_size = 8\n",
    "threshold = math.ceil((num_nodes / (top_k+1) ) * 5)\n",
    "epoch = int(len(testset_tsp)/batch_size)\n",
    "buff_coor = np.zeros(shape=(num_nodes, 2), dtype = np.float64)\n",
    "start_row_num = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# init\n",
    "K_expand = 149\n",
    "count_buff = np.zeros(shape=(batch_size*threshold, ), dtype=np.int32)\n",
    "edges = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "edges_values = np.zeros(shape=(batch_size*threshold, K+1, K+1), dtype=np.float16)\n",
    "nodes = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "nodes_coord = np.zeros(shape = (batch_size*threshold, K+1, 2), dtype=np.float16)\n",
    "edges_target = np.zeros(shape = (batch_size*threshold, K+1, K+1), dtype=np.int32)\n",
    "nodes_target = np.zeros(shape = (batch_size*threshold, K+1), dtype=np.int32)\n",
    "meshs = np.zeros(shape = (batch_size*threshold, 2, K+1, K+1), dtype=np.int32)\n",
    "Omegas = np.zeros(shape = (batch_size, num_nodes, num_nodes), dtype=np.int32)\n",
    "opts = np.zeros(shape = (batch_size, num_nodes+1), dtype=np.int32)\n",
    "num_neighbors = config.num_neighbors\n",
    "beam_size = config.beam_size\n",
    "\n",
    "sum_time = 0\n",
    "for j in tqdm.tqdm(range(epoch)):\n",
    "    start = time.time()\n",
    "    for i in range(batch_size):\n",
    "        edge, edges_value, node, node_coord, edge_target, node_target, mesh, omega, opt = test_one_tsp(tsp_source=testset_tsp[start_row_num+i], \n",
    "                                                                                      coor_buff=buff_coor, node_num=num_nodes, \n",
    "                                                                                      cluster_center=0, top_k=K, top_k_expand=K_expand)\n",
    "        edges[i*threshold:(i+1)*threshold, ...] = edge\n",
    "        edges_values[i*threshold:(i+1)*threshold, ...] = edges_value\n",
    "        nodes[i*threshold:(i+1)*threshold, ...] = node\n",
    "        nodes_coord[i*threshold:(i+1)*threshold, ...] = node_coord\n",
    "        edges_target[i*threshold:(i+1)*threshold, ...] = edge_target\n",
    "        nodes_target[i*threshold:(i+1)*threshold, ...] = node_target\n",
    "        meshs[i*threshold:(i+1)*threshold, ...] = mesh\n",
    "        Omegas[i, ...] = omega\n",
    "        opts[i, ...] = opt\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = Variable(torch.LongTensor(edges).type(dtypeLong), requires_grad=False)\n",
    "        x_edges_values = Variable(torch.FloatTensor(edges_values).type(dtypeFloat), requires_grad=False)\n",
    "        x_nodes = Variable(torch.LongTensor(nodes).type(dtypeLong), requires_grad=False)\n",
    "        x_nodes_coord = Variable(torch.FloatTensor(nodes_coord).type(dtypeFloat), requires_grad=False)\n",
    "        y_edges = Variable(torch.LongTensor(edges_target).type(dtypeLong), requires_grad=False)\n",
    "        y_nodes = Variable(torch.LongTensor(nodes_target).type(dtypeLong), requires_grad=False)\n",
    "\n",
    "        # Compute class weights\n",
    "        edge_labels = y_edges.cpu().numpy().flatten()\n",
    "        edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        y_preds_prob = F.softmax(y_preds, dim=3)\n",
    "        y_preds_prob_numpy = y_preds_prob.cpu().numpy()\n",
    "\n",
    "    # multi - processes\n",
    "#     progress_pool = Pool(processes=10)\n",
    "#     for i in range(batch_size):\n",
    "#         heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "#         progress_pool.apply_async(multiprocess_write, args=(y_preds_prob_numpy[i*thre:(i+1)*thre, ...],\n",
    "#                                                            meshs[i*thre:(i+1)*thre, ...], Omegas[i, ...],\n",
    "#                                                            num_nodes, heatmap_path, True, opts[i, ...]))\n",
    "#     progress_pool.close()\n",
    "#     progress_pool.join()\n",
    "    end = time.time()\n",
    "    sum_time += end - start\n",
    "    # single - process\n",
    "    for i in range(batch_size):\n",
    "        heatmap_path = f'./results/heatmap/tsp{num_nodes}/heatmaptsp{num_nodes}_{i+start_row_num}.txt'\n",
    "        rank = multiprocess_write(y_preds_prob_numpy[i*threshold:(i+1)*threshold, ...],\n",
    "                                                           meshs[i*threshold:(i+1)*threshold, ...], Omegas[i, ...],\n",
    "                                                           num_nodes, heatmap_path, True, opts[i, ...])\n",
    "        avg_mean_rank.append(rank)\n",
    "    start_row_num+= batch_size\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tsp_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "febb4a57051e069b856d08f5e455afb0caa21b93624ae2a89969291f04104718"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}